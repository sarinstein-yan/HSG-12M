# HSG-12M

HSG-12M Spatial Multigraph Dataset.

This repository contains the code in the companion paper "HSG-12M: A Large-Scale Spatial Multigraph Dataset":
- the code for generation of HSG-12M
- the code to derive the six dataset variants, and generic custom subsets
- the code for preliminary featurization and processing to PyTorch Geometric Dataset (both in memory and on disk)
- the code for benchmarking on GNN baseline models

The 1401 data files are publicly available at [Dataverse](https://doi.org/10.7910/DVN/PYDSSQ).

The dataset is generated by [`poly2graph`](https://github.com/sarinstein-yan/poly2graph).

See the [`tutorial.ipynb`](https://github.com/sarinstein-yan/HSG-12M/blob/main/tutorial.ipynb) for an interactive start.


## Installation
The package requires `python>=3.11` and can be installed locally.
```bash
$ conda create -n hsg python=3.12 # python>=3.11
$ conda activate hsg

$ git clone https://github.com/sarinstein-yan/HSG-12M.git
$ cd HSG-12M
$ pip install . 
```
Or if you want to install with a specific CUDA version of PyTorch, you can set the `CUDA` environment variable to the desired version (e.g., `cu124` for CUDA 12.4) before running the installation command:
```bash
$ export CUDA=cu124 # < On Linux or macOS
# On Windows (PowerShell): `$CUDA = "cu124"`
$ pip install . --extra-index-url https://download.pytorch.org/whl/${CUDA}
```


## Raw Data Files

The download of raw data files is by `easyDataverse`, which sometimes encounters timeout connecting to Dataverse. Retry or connect to better network if you encounter timeout when initializing the dataset.

```python
from easyDataverse import Dataverse

server_url = "https://dataverse.harvard.edu"
dataset_pid = "doi:10.7910/DVN/PYDSSQ"

dv = Dataverse(server_url)
hsg = dv.load_dataset(
    pid=dataset_pid, 
    # local directory to save files
    filedir='.',
    # which classes to download
    filenames=[f'raw/class_{i}.npz' for i in range(1401)],
    # Set to True to download the files, requires at least 257GB of free space
    download_files=False,
)

# metadata
print(hsg.citation)
```

Take the **9-th** class as an example, if the `raw/class_9.npz` file is downloaded, one can load the `nx.MultiGraph` objects, class label `y`, the parameter values `a_vals`, `b_vals`, and the class-specific metadata as follows:

```python
import hsg

nx_graphs, y, a_vals, b_vals, class_metas = hsg.load_class(class_idx=9, raw_dir='./dev/raw')

print("class label:", y)
print("a_vals:", a_vals)
print("b_vals:", b_vals)
print("polynomial latex:", class_metas['latex'])
print("polynomial parameter symbols:", class_metas['parameter_symbols'])
print("polynomial generators:", class_metas['generator_symbols'])
print("number of bands:", class_metas['number_of_bands'])
print("max left hopping:", class_metas['max_left_hopping'])
print("max right hopping:", class_metas['max_right_hopping'])
import sympy as sp
poly = sp.sympify(class_metas['sympy_repr'])
poly
```
<span style="color:#d73a49;font-weight:bold">>>></span>
```text
class label: 9
a_vals: [-10.-5.j -10.-2.j -10.-1.j ...  10.+1.j  10.+2.j  10.+5.j]
b_vals: [-10.-5.j -10.-5.j -10.-5.j ...  10.+5.j  10.+5.j  10.+5.j]
polynomial latex: - E + \frac{a}{z} + b z + z^{2} + \frac{1}{z^{2}}
polynomial parameter symbols: ['a' 'b']
polynomial generators: ['z' '1/z' 'E']
number of bands: 1
max left hopping: 2
max right hopping: 2
```
$$\text{Poly}{\left( z^{2} + b z + \frac{1}{z^{2}} + a \frac{1}{z} - E, z, \frac{1}{z}, E, domain=\mathbb{Z}\left[a, b\right] \right)}$$


## PyG Datasets

The download of raw data files is by [`easyDataverse`](https://github.com/gdcc/easyDataverse), which sometimes encounters timeout connecting to `Dataverse`. Retry or connect to better network if you encounter timeout when initializing the dataset.

The `NetworkX MultiGraph` dataset is processed to [`PyTorch Geometric`](https://pyg.org/) Dataset via the scheme discussed in the companion paper. 

Feel free to overwrite the `process` method in `HSGOnDisk` or `HSGInMemory` to customize the featurization and processing.

```python
from hsg import HSGOnDisk, HSGInMemory

# # use HSGOnDisk if some large dataset variant overflows RAM
# ds = HSGOnDisk(root="./dev", subset="one-band")

ds = HSGInMemory(root="./dev", subset="one-band")

print("Number of graphs:", len(ds))
print("Number of classes:", ds.num_classes)
print("Number of node features:", ds.num_features)
print("Number of edge features:", ds.num_edge_features)
print("First graph:", ds[12345])
```
<span style="color:#d73a49;font-weight:bold">>>></span>
```text
Number of graphs: 198744
Number of classes: 24
Number of node features: 4
Number of edge features: 13
First graph: Data(edge_index=[2, 6], x=[4, 4], edge_attr=[6, 13], y=[1])
```


## Benchmarking

Training is done with [`PyTorch Lightning`](https://lightning.ai/docs/pytorch/stable/). The training code is located in `src/hsg/training.py`.

To run the benchmarking on the GNN baseline models:

```bash
$ python -m src/hsg/training.py \
    --root path/to/dataset_root \
    --save_dir path/to/baseline_result \
    --subset one-band \
    # ^ available: one-band, two-band, three-band, topology, all (all-static)
    --models gcn sage gat gin \
    # ^ available: gcn, sage, gat, gatv2, gin, gine; omit for all
    --epochs 200 \
    --batch_size 256 \
    --dim_gnn 256 \
    --dim_mlp 256 \
    --layers_gnn 4 \
    --layers_mlp 2 \
    --heads 1 \
    # ^ only for GAT and GATv2
    --dropout 0.1 \
    --lr_init 1e-2 \
    --lr_min 1e-5 \
    --t0 10 \
    --t_mult 4 \
    --seeds 42 624 706 \
    --log_every_n_steps 5 \
    --early_stop_patience 10
```

To run interactively, use `hsg.run_experiment(args: argparse.Namespace)` (source in `src/hsg/training.py`) to run the experiment interactively (in IPython or Jupyter Notebook):

```python
from argparse import Namespace
args = Namespace(
    root='path/to/dataset_root',
    save_dir='path/to/baseline_result',
    subset='one-band',
    models=['gcn', 'sage', 'gat', 'gin'],
    ...
)
hsg.run_experiment(args)
```

To view Tensorboard logs:
```bash
$ tensorboard --logdir path/to/baseline_result
```

## HSG-12M Generation

The dataset generator used in the companion paper is as follows:

```python
import hsg
gen = hsg.HSG_Generator(
    root="./dev",
    hopping_range=[4,5,6], 
    num_bands=[1,2,3],
    real_coeff_walk=[-10, -5, -2, -1, -0.5, -0.1, 0, 0.1, 0.5, 1, 2, 5, 10],
    imag_coeff_walk=[-5, -2, -1, 0, 1, 2, 5],
)
num_classes = len(gen.all_metas)
```

Run the following to generate the 1401 raw data files of HSG-12M:

```python
for i in range(num_classes):
    gen.generate_dataset(
            class_idx=i,
            num_partition=20,
            # ^ generate the class in 20 partitions, can set to 1 if RAM is large enough
            short_edge_threshold=30,
            # ^ merge near-by nodes within this distance threshold, see `poly2graph` documentation
        )
```

## Load `T-HSG-5.1M`

Again, take the **9-th** class as an example, to obtain the temporal graphs derived from class 9:

```python
tg_9 = gen.get_temporal_graphs_by_class(class_idx=9)
```

To get the whole `T-HSG-5.1M` as a `List[List[networkx.MultiGraph]]`:

```python
thsg = []
y = []
from tqdm import tqdm
for i in tqdm(range(num_classes)):
    tg_i = gen.get_temporal_graphs_by_class(i)
    thsg.extend(tg_i)
    y.extend([i] * len(tg_i))

print("Number of temporal graphs:", len(thsg))
```